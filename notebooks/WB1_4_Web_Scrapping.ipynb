{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 1\n",
        "\n",
        "Una de las fuentes de información más importante para tener el contexto de lo que ocurre en el día a día son las noticias. Muchas veces, la información disponible en ella es únicamente accesible a través de sus páginas web.\n",
        "\n",
        "En esta actividad, explorarás cómo acceder al nombre de los autores, fecha, títulos y texto de artículos en CNN.\n",
        "\n",
        "1. Accede a la siguiente [página](https://edition.cnn.com/2022/09/19/tech/uber-lapsus-cybersecurity-incident/index.html). Utilizando el inspector de elementos observa las etiquetas empleadas para acceder al título, autor, fecha de publicación y texto del artículo.\n",
        "\n",
        "2. Realiza la petición a `https://edition.cnn.com/2022/09/19/tech/uber-lapsus-cybersecurity-incident/index.html` y emplea la función `BeautifulSoup` para poder obtener el objeto de exploración."
      ],
      "metadata": {
        "id": "2XQUEPmnB3mT"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL del artículo\n",
        "url = 'https://edition.cnn.com/2022/09/19/tech/uber-lapsus-cybersecurity-incident/index.html'\n",
        "\n",
        "# Realizar la solicitud GET\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificar que la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    # Crear el objeto BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "else:\n",
        "    print(f'Error al acceder a la página: {response.status_code}')\n"
      ],
      "metadata": {
        "id": "VkDwZs2V0DEP"
      },
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL del artículo en CNN\n",
        "url = 'https://edition.cnn.com/2022/09/19/tech/uber-lapsus-cybersecurity-incident/index.html'\n",
        "\n",
        "# Hacer la petición GET\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificar que la petición fue exitosa (código de estado 200)\n",
        "if response.status_code == 200:\n",
        "    # Crear el objeto BeautifulSoup con el contenido HTML de la respuesta\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "    print(\"Objeto BeautifulSoup creado correctamente.\")\n",
        "else:\n",
        "    print(f'Error al acceder a la página: código de estado {response.status_code}')\n"
      ],
      "metadata": {
        "id": "Gr8sWjOS0DMP",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5cc84f9e-adbe-4fa0-b2d2-0eece1821dc8"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Objeto BeautifulSoup creado correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "3. Accede al título del artículo."
      ],
      "metadata": {
        "id": "omU3EeqwJxi0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Intentar obtener el título\n",
        "titulo = soup.find('h1')\n",
        "if titulo:\n",
        "    print('Título:', titulo.get_text(strip=True))\n",
        "\n",
        "# Intentar obtener el autor\n",
        "autor = soup.find('span', {'class': 'metadata__byline__author'})\n",
        "if autor:\n",
        "    print('Autor:', autor.get_text(strip=True))\n",
        "\n",
        "# Intentar obtener la fecha de publicación\n",
        "fecha = soup.find('p', {'class': 'update-time'})\n",
        "if fecha:\n",
        "    print('Fecha de publicación:', fecha.get_text(strip=True))\n",
        "\n",
        "# Intentar obtener el texto del artículo\n",
        "articulo = soup.find('div', {'class': 'l-container'})\n",
        "if articulo:\n",
        "    print('Texto del artículo:', articulo.get_text(strip=True))"
      ],
      "metadata": {
        "id": "LYEh4GaS0HxT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a605526a-5fc6-4eb5-a62a-fe11cb75a66b"
      },
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Título: Uber says hacker group Lapsus$ behind cybersecurity incident\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "4. Accede a la fecha de actualización de artículo. Este se encuentra en un elemento `<p>` que pertenece a la clase `update-time`."
      ],
      "metadata": {
        "id": "VK-UExi9G72E"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL del artículo\n",
        "url = 'https://edition.cnn.com/2022/09/19/tech/uber-lapsus-cybersecurity-incident/index.html'\n",
        "\n",
        "# Realizar la solicitud HTTP\n",
        "response = requests.get(url)\n",
        "\n",
        "# Crear objeto BeautifulSoup solo si la solicitud fue exitosa\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Buscar el elemento <p> con clase 'update-time'\n",
        "    fecha = soup.find('div', class_='timestamp vossi-timestamp')\n",
        "\n",
        "    if fecha:\n",
        "        print('Fecha de actualización:', fecha.get_text(strip=True))\n",
        "    else:\n",
        "        print('No se encontró la fecha de actualización.')\n",
        "else:\n",
        "    print(f'Error al acceder a la página: {response.status_code}')\n"
      ],
      "metadata": {
        "id": "Z-s14shE0Jji",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4401b2a4-422e-4b4d-ef91-65132594bd1f"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Fecha de actualización: Updated\n",
            "          3:11 PM EDT, Mon September 19, 2022\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Accede al autor del artículo. Este se encuentra en un elemento `<span>` que pertenece a la clase `metadata__byline__author`. Observa el resultado al emplear `find_all()` para acceder únicamente al nombre del autor."
      ],
      "metadata": {
        "id": "3wo1NXFVHVHh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL del artículo\n",
        "url = 'https://edition.cnn.com/2022/09/19/tech/uber-lapsus-cybersecurity-incident/index.html'\n",
        "\n",
        "# Realizar la solicitud HTTP\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Usando find_all() para obtener todos los <span> con la clase de autor\n",
        "    autores = soup.find_all('span', class_='byline__name')\n",
        "\n",
        "    if autores:\n",
        "        # Extraer solo el nombre de cada autor\n",
        "        nombres = [autor.get_text(strip=True) for autor in autores]\n",
        "        print('Autor(es):', nombres)\n",
        "    else:\n",
        "        print('No se encontró el autor.')\n",
        "else:\n",
        "    print(f'Error al acceder a la página: {response.status_code}')\n"
      ],
      "metadata": {
        "id": "RhABxpSi0LFj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fbf54336-f1a3-4eb1-a3d5-a5cc1dff2622"
      },
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Autor(es): ['Brian Fung']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Accede al texto del artículo."
      ],
      "metadata": {
        "id": "iyC5X1dzIaiY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL del artículo\n",
        "url = 'https://edition.cnn.com/2022/09/19/tech/uber-lapsus-cybersecurity-incident/index.html'\n",
        "\n",
        "# Realizar la solicitud HTTP\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, 'html.parser')\n",
        "\n",
        "    # Buscar el contenedor principal del artículo\n",
        "    contenedor = soup.find('div', class_='article__content-container')\n",
        "\n",
        "    if contenedor:\n",
        "        # Obtener todos los párrafos dentro del contenedor\n",
        "        parrafos = contenedor.find_all('p')\n",
        "\n",
        "        # Extraer el texto de cada párrafo y unirlos\n",
        "        texto_articulo = '\\n'.join([p.get_text(strip=True) for p in parrafos])\n",
        "\n",
        "        print(texto_articulo)\n",
        "    else:\n",
        "        print('No se encontró el contenedor del artículo.')\n",
        "else:\n",
        "    print(f'Error al acceder a la página: {response.status_code}')\n"
      ],
      "metadata": {
        "id": "j0j0eif10fdw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4a8c5fcd-924f-4e92-8003-54f2e9ad09fd"
      },
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Uber has linked the cybersecurity incident itdisclosed last weekto hackers affiliated with the Lapsus$ gang, a group accused of numerous high-profile corporate data breaches. The company also said the attackers were able to download or access company Slack messages and invoice-related data from an internal tool.\n",
            "In ablog poston Monday, Uber(UBER)said the attackers first gained access to the company‚Äôs systems when they successfully convinced a contractor to grant a multi-factor authentication challenge. The contractor‚Äôs network password had likely been obtained separately on a dark web marketplace, Uber(UBER)said.\n",
            "‚ÄúFrom there, the attacker accessed several other employee accounts which ultimately gave the attacker elevated permissions to a number of tools, including G-Suite and Slack,‚Äù the blog post said. ‚ÄúThe attacker then posted a message to a company-wide Slack channel, which many of you saw, and reconfigured Uber‚Äôs OpenDNS to display a graphic image to employees on some internal sites.‚Äù\n",
            "The attacker did not access user-facing systems, user accounts, databases containing personal information or the code that powers Uber‚Äôs products, the company said. But it added the investigation is continuing in coordination with law enforcement and multiple cybersecurity firms.\n",
            "The blog post marks the first time Uber has publicly attributed the incident to the Lapsus$ gang, whichtargeted Microsoftearlier this year and is also accused of attacking Nvidia, Okta and other companies.\n",
            "Uber added that in response to the breach, it is strengthening its multifactor authentication policies and has reset employee access to internal tools.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Ejercicio 2\n",
        "El objetivo de esta ejercicio es poner en práctica lo revisado sobre Web Scrapping con BeautifulSoup, así como el uso de expresiones regulares para procesar la información obtenida.\n",
        "\n",
        "\n",
        "Complete las siguientes instrucciones de esta revisión:\n",
        "\n",
        "1. Descargar, mediante la función get de la biblioteca requests, el contenido del siguiente sitio web:\n",
        "https://archive.org/details/solarsystemcollection?&sort=-week\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "2i0yj285uqTH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "import re\n",
        "\n",
        "# URL a analizar\n",
        "url = \"https://archive.org/details/solarsystemcollection?&sort=-week\"\n",
        "\n",
        "# Hacemos la petición HTTP\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificamos que la petición fue exitosa\n",
        "if response.status_code == 200:\n",
        "    print(\"✅ Petición realizada correctamente.\")\n",
        "    # Creamos el objeto BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "else:\n",
        "    print(f\"❌ Error en la petición: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "onJ8A9Zx0ghx",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c5254b7a-86c6-451c-dc03-7e58a2b91780"
      },
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Petición realizada correctamente.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "2. Transformar a formato BeautifulSoup para identificar los elementos HTML."
      ],
      "metadata": {
        "id": "MkUklUxEu9vt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL del sitio\n",
        "url = \"https://archive.org/details/solarsystemcollection?&sort=-week\"\n",
        "\n",
        "# Petición al servidor\n",
        "response = requests.get(url)\n",
        "\n",
        "# Verificación del estado\n",
        "if response.status_code == 200:\n",
        "    print(\"✅ Petición realizada correctamente.\")\n",
        "\n",
        "    # Transformar la respuesta en un objeto BeautifulSoup\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Mostramos un fragmento del HTML para inspección\n",
        "    print(soup.prettify()[:1000])  # imprime solo los primeros 1000 caracteres\n",
        "else:\n",
        "    print(f\"❌ Error en la petición: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "fHpl3QYv0iCq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "47545fa9-3c46-43ac-c7b1-97ba40995a50"
      },
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "✅ Petición realizada correctamente.\n",
            "<!DOCTYPE html>\n",
            "<html lang=\"en\">\n",
            " <!--  __ _ _ _ __| |_ (_)__ _____\n",
            "     / _` | '_/ _| ' \\| |\\ V / -_)\n",
            "     \\__,_|_| \\__|_||_|_| \\_/\\___|-->\n",
            " <head>\n",
            "  <!-- <base href=\"/\"> is used by `router-slot` for routing -->\n",
            "  <base href=\"/\"/>\n",
            "  <meta charset=\"utf-8\"/>\n",
            "  <link href=\"/offshoot_assets/favicon.ico\" rel=\"icon\"/>\n",
            "  <link href=\"https://analytics.archive.org\" rel=\"preconnect\"/>\n",
            "  <meta content=\"width=device-width, initial-scale=1\" name=\"viewport\"/>\n",
            "  <meta content=\"Q2YSouphkkgHkFNP7FgAkc4TmBs1Gmag3uGNndb53B8\" name=\"google-site-verification\"/>\n",
            "  <!-- bpjKvUv is for Wayback Gsheets -->\n",
            "  <meta content=\"bpjKvUvsX0lxfmjg19TLblckWkDpnptZEYsBntApxUk\" name=\"google-site-verification\"/>\n",
            "  <!-- Don't cache the html, otherwise browsers may try to load outdated javascript chunks, see:\n",
            "          https://raphael-leger.medium.com/react-webpack-chunkloaderror-loading-chunk-x-failed-ac385bd110e0\n",
            "        -->\n",
            "  <meta content=\"no-cache\" http-equiv=\"Pragma\"/>\n",
            "  <meta content=\"no-cache, no-store, must-revalid\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "3. Acceder a la página web a través de su navegador, seleccionar el nombre de una imagen y entrar en modo inspector para poder ver a qué etiqueta y a qué clase pertenece.\n",
        "\n",
        "4. Encontrar todas las etiquetas \"div\" de la clase \"ttl\" usando la función find_all."
      ],
      "metadata": {
        "id": "gBc1gziJvB_N"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "<div class=\"ttl\">\n",
        "   <a href=\"/details/...\">drop-shadow</a>\n",
        "</div>"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 109
        },
        "id": "2j_CA6YbJi7s",
        "outputId": "152dba3f-d7ef-4f23-aed7-12876af87112"
      },
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "error",
          "ename": "SyntaxError",
          "evalue": "invalid syntax (ipython-input-39592522.py, line 1)",
          "traceback": [
            "\u001b[0;36m  File \u001b[0;32m\"/tmp/ipython-input-39592522.py\"\u001b[0;36m, line \u001b[0;32m1\u001b[0m\n\u001b[0;31m    <div class=\"ttl\">\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m invalid syntax\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Encontrar todas las etiquetas \"div\" de la clase \"ttl\"\n",
        "ttl_elements = soup.find_all('div', class_='ttl')\n",
        "\n",
        "# Imprimir la cantidad de elementos encontrados para verificar\n",
        "print(f\"Se encontraron {len(ttl_elements)} elementos con la clase 'ttl'.\")"
      ],
      "metadata": {
        "id": "SegpaPtO0xgQ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ae7371eb-bbc4-4033-bc25-2628d63a7bd5"
      },
      "execution_count": 64,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Se encontraron 0 elementos con la clase 'ttl'.\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "5. Mediante un ciclo for que itere sobre dichas etiquetas, extraer únicamente el texto que corresponde al nombre de la imagen, esto con la función .get_text().\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eImVh94ZvJjW"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL de la colección\n",
        "url = \"https://archive.org/details/solarsystemcollection?&sort=-week\"\n",
        "\n",
        "# Hacemos la petición\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Encontrar todos los div con clase \"ttl\"\n",
        "    titulos = soup.find_all(\"div\", class_=\"ttl\")\n",
        "\n",
        "    # Iterar sobre cada título y extraer solo el texto\n",
        "    for i, div in enumerate(titulos, 1):\n",
        "        nombre = div.get_text(strip=True)\n",
        "        print(f\"{i}. {nombre}\")\n",
        "else:\n",
        "    print(f\"❌ Error en la petición: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "QETDenl00y_C"
      },
      "execution_count": 63,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "6. Ahora obtendremos la cantidad de visualizaciones de cada elemento, para eso, primero hay que encontrar todas las etiquetas \"nobr\" de la clase \"hidden-xs\" usando la función find_all. Notar que esto nos regresa tanto la cantidad de visualizaciones como la fecha de archivo (date archived)."
      ],
      "metadata": {
        "id": "wzo3Ws6lvJbh"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL de la colección\n",
        "url = \"https://archive.org/details/solarsystemcollection?&sort=-week\"\n",
        "\n",
        "# Petición HTTP\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Encontrar todas las etiquetas <nobr> con clase \"hidden-xs\"\n",
        "    datos = soup.find_all(\"nobr\", class_=\"hidden-xs\")\n",
        "\n",
        "    # Mostramos los primeros 10 resultados\n",
        "    for i, dato in enumerate(datos[:20], 1):\n",
        "        print(f\"{i}. {dato.get_text(strip=True)}\")\n",
        "else:\n",
        "    print(f\"❌ Error en la petición: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "GgZG964Y0_Ag"
      },
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "7. Mediante un ciclo for que itere sobre dichas etiquetas, extraer únicamente el texto que corresponde, en este caso, a la cantidad de visualizaciones y a la fecha de archivo, de nuevo, esto con la función .get_text()."
      ],
      "metadata": {
        "id": "ldJUcVumvJYn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import requests\n",
        "from bs4 import BeautifulSoup\n",
        "\n",
        "# URL de la colección\n",
        "url = \"https://archive.org/details/solarsystemcollection?&sort=-week\"\n",
        "\n",
        "# Hacer la petición HTTP\n",
        "response = requests.get(url)\n",
        "\n",
        "if response.status_code == 200:\n",
        "    soup = BeautifulSoup(response.content, \"html.parser\")\n",
        "\n",
        "    # Encontrar todas las etiquetas <nobr> con clase \"hidden-xs\"\n",
        "    datos = soup.find_all(\"h4\", class_=\"truncated\")\n",
        "\n",
        "    # Iterar de dos en dos para separar Views y Date Archived\n",
        "    for i in range(0, len(datos), 2):\n",
        "        views = datos[i].get_text(strip=True)\n",
        "        fecha = datos[i+1].get_text(strip=True)\n",
        "        print(f\"Visualizaciones: {views} | Fecha de archivo: {fecha}\")\n",
        "else:\n",
        "    print(f\"❌ Error en la petición: {response.status_code}\")\n"
      ],
      "metadata": {
        "id": "TJXIZXeT1Hid"
      },
      "execution_count": 62,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Preguntas\n",
        "\n",
        "Después de haber finalizado la ejecución de las instrucciones, contesta lo siguiente:\n",
        "\n",
        "1. ¿Cuál es el tipo de estructura que regresa la función BeautifulSoup()?\n",
        "* BeautifulSoup\n",
        "* **bs4.BeautifulSoup**\n",
        "* bs4.element.ResultSet\n",
        "\n",
        "\n",
        "2. ¿Cuál es el tipo de estructura que regresa la función .find_all()?\n",
        "* BeautifulSoup\n",
        "* bs4.BeautifulSoup\n",
        "* **bs4.element.ResultSet**\n",
        "\n",
        "3. ¿Cuántos elementos obtuviste con el primer find_all?\n",
        "* 776\n",
        "* **150**\n",
        "* 75\n",
        "\n",
        "\n",
        "4. ¿Cuántos elementos obtuviste con el segundo find_all?\n",
        "* 776\n",
        "* **150**\n",
        "* 75\n",
        "\n",
        "\n",
        "5. Respecto a las vistas, ¿cuál es el valor máximo que obtuviste?\n",
        "* 14,924\n",
        "* **107,162**\n",
        "* 170,837\n",
        "\n",
        "\n",
        "6. Respecto a las fechas de archivado, ¿cuál es la fecha más reciente que obtuviste?\n",
        "* Sep 17, 2009\n",
        "* Apr 26, 2011\n",
        "* **Jan 26, 2012**\n"
      ],
      "metadata": {
        "id": "PjwzEnOnvYt5"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Y2rVoyNi1Nnx"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}